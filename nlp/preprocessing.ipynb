{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前處理\n",
    "\n",
    "自然語言處理在使用機器學習模型之前，需要一些像是斷字、斷句等前處理(preprocessing)的工作。\n",
    "\n",
    "- [斷字](#斷字)\n",
    "  - [split by space](#split-by-space)\n",
    "  - [nltk.tokenize](#nltk.tokenize)\n",
    "  - [sklearn.CountVectorizer](#sklearn.CountVectorizer)\n",
    "  - [keras.text_to_word_sequence](#keras.text_to_word_sequence) (推薦)\n",
    "- [單字轉向量 / Word to Vector](#單字轉向量-/-Word-to-Vector)\n",
    "  - [sklearn.LabelEncoder](#sklearn.LabelEncoder)\n",
    "  - [sklearn.OneHotEncoder](#sklearn.OneHotEncoder)\n",
    "  - [keras.to_categorical](#keras.to_categorical)\n",
    "- [文件轉向量 / Document to Vector](#文件轉向量-/-Document-to-Vector)\n",
    "  - [sklearn.CountVectorizer](#sklearn.CountVectorizer)\n",
    "  - [sklearn.TfidfVectorizer](#sklearn.TfidfVectorizer)\n",
    "  - [sklearn.TruncatedSVD](#sklearn.TruncatedSVD)\n",
    "  - [sklearn.PCA](#sklearn.PCA)\n",
    "\n",
    "# 斷字\n",
    "\n",
    "以下面 `text` 的文字為例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Yeah, I know when I compliment her she won't believe me \\\n",
    "        And it's so sad to think that she don't see what I see \\\n",
    "        But every time she asks me do I look okay? \\\n",
    "        I say When I see your face \\\n",
    "        There's not a thing that I would change \\\n",
    "        Cause you're amazing Just the way you are\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yeah,', 'I', 'know', 'when', 'I', 'compliment', 'her', 'she', \"won't\", 'believe', 'me', 'And', \"it's\", 'so', 'sad', 'to', 'think', 'that', 'she', \"don't\", 'see', 'what', 'I', 'see', 'But', 'every', 'time', 'she', 'asks', 'me', 'do', 'I', 'look', 'okay?', 'I', 'say', 'When', 'I', 'see', 'your', 'face', \"There's\", 'not', 'a', 'thing', 'that', 'I', 'would', 'change', 'Cause', \"you're\", 'amazing', 'Just', 'the', 'way', 'you', 'are']\n"
     ]
    }
   ],
   "source": [
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.tokenize\n",
    "\n",
    "- 逗號、問號變成獨立的字\n",
    "- 縮寫 `n't`、`'s` 與 `'re` 被斷成獨立的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ./nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['Yeah', ',', 'I', 'know', 'when', 'I', 'compliment', 'her', 'she', 'wo', \"n't\", 'believe', 'me', 'And', 'it', \"'s\", 'so', 'sad', 'to', 'think', 'that', 'she', 'do', \"n't\", 'see', 'what', 'I', 'see', 'But', 'every', 'time', 'she', 'asks', 'me', 'do', 'I', 'look', 'okay', '?', 'I', 'say', 'When', 'I', 'see', 'your', 'face', 'There', \"'s\", 'not', 'a', 'thing', 'that', 'I', 'would', 'change', 'Cause', 'you', \"'re\", 'amazing', 'Just', 'the', 'way', 'you', 'are']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from os.path import abspath\n",
    "\n",
    "# prepare `punkt` corpus\n",
    "nltk.download('punkt', download_dir='./nltk_data')\n",
    "nltk.data.path = [abspath('./nltk_data')]\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.CountVectorizer\n",
    "\n",
    "- 移除逗號、問號\n",
    "- 縮寫 `'t` 與 `'s` 被移除；但縮寫 `'re` 被斷成獨立的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yeah', 'know', 'when', 'compliment', 'her', 'she', 'won', 'believe', 'me', 'And', 'it', 'so', 'sad', 'to', 'think', 'that', 'she', 'don', 'see', 'what', 'see', 'But', 'every', 'time', 'she', 'asks', 'me', 'do', 'look', 'okay', 'say', 'When', 'see', 'your', 'face', 'There', 'not', 'thing', 'that', 'would', 'change', 'Cause', 'you', 're', 'amazing', 'Just', 'the', 'way', 'you', 'are']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "print(CountVectorizer().build_tokenizer()(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras.text_to_word_sequence\n",
    "\n",
    "- 全轉小寫\n",
    "- 移除逗號、問號\n",
    "- 保留縮寫\n",
    "\n",
    "看起來效果最好，不過速度最慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeah', 'i', 'know', 'when', 'i', 'compliment', 'her', 'she', \"won't\", 'believe', 'me', 'and', \"it's\", 'so', 'sad', 'to', 'think', 'that', 'she', \"don't\", 'see', 'what', 'i', 'see', 'but', 'every', 'time', 'she', 'asks', 'me', 'do', 'i', 'look', 'okay', 'i', 'say', 'when', 'i', 'see', 'your', 'face', \"there's\", 'not', 'a', 'thing', 'that', 'i', 'would', 'change', 'cause', \"you're\", 'amazing', 'just', 'the', 'way', 'you', 'are']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 單字轉向量 / Word to Vector\n",
    "\n",
    "以下面 `words` 的單字串列為例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['cute', 'beauty', 'cold', 'cold', 'cold', 'hot', 'beauty', 'cute']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 1 1 1 3 0 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "word_ids = le.fit_transform(words)\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot = OneHotEncoder(sparse=False)\n",
    "print(one_hot.fit_transform(word_ids.reshape(len(word_ids), 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras.to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "print(to_categorical(word_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件轉向量 / Document to Vector\n",
    "\n",
    "以下面 `docs` 的文件串列為例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['bobo is cute', \\\n",
    "        'bobo is smart', \\\n",
    "        'bobo is humorous']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bobo', 'cute', 'humorous', 'is', 'smart']\n",
      "[[1 1 0 1 0]\n",
      " [1 0 0 1 1]\n",
      " [1 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "doc_term = cv.fit_transform(docs)\n",
    "print(cv.get_feature_names())\n",
    "print(doc_term.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.TfidfVectorizer\n",
    "\n",
    "- tf: term frequency\n",
    "- idf: inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bobo', 'cute', 'humorous', 'is', 'smart']\n",
      "[[ 0.45329466  0.76749457  0.          0.45329466  0.        ]\n",
      " [ 0.45329466  0.          0.          0.45329466  0.76749457]\n",
      " [ 0.45329466  0.          0.76749457  0.45329466  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer()\n",
    "doc_term = tv.fit_transform(docs)\n",
    "print(tv.get_feature_names())\n",
    "print(doc_term.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.77929545  0.          0.62665669]\n",
      " [ 0.77929545 -0.54270061 -0.31332835]\n",
      " [ 0.77929545  0.54270061 -0.31332835]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=3)\n",
    "print(svd.fit_transform(doc_term.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.26656690e-01   4.67720313e-17   1.54037282e-16]\n",
      " [ -3.13328345e-01  -5.42700613e-01   1.54037282e-16]\n",
      " [ -3.13328345e-01   5.42700613e-01   1.54037282e-16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3, svd_solver='full')\n",
    "print(pca.fit_transform(doc_term.toarray()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
