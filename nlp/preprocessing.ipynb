{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前處理\n",
    "\n",
    "自然語言處理在使用機器學習模型之前，需要一些像是斷字、斷句等前處理(preprocessing)的工作。\n",
    "\n",
    "- [斷字](#斷字)\n",
    "  - [split by space](#split-by-space)\n",
    "  - [nltk.tokenize](#nltk.tokenize)\n",
    "  - [sklearn.CountVectorizer](#sklearn.CountVectorizer)\n",
    "  - [keras.text_to_word_sequence](#keras.text_to_word_sequence) (推薦)\n",
    "- [單字轉向量 / Word to Vector](#單字轉向量-/-Word-to-Vector)\n",
    "  - [sklearn.LabelEncoder](#sklearn.LabelEncoder)\n",
    "  - [sklearn.OneHotEncoder](#sklearn.OneHotEncoder)\n",
    "  - [keras.to_categorical](#keras.to_categorical)\n",
    "- [文件轉向量 / Document to Vector](#文件轉向量-/-Document-to-Vector)\n",
    "  - [sklearn.CountVectorizer](#sklearn.CountVectorizer)\n",
    "  - [sklearn.TfidfVectorizer](#sklearn.TfidfVectorizer)\n",
    "  - [sklearn.TruncatedSVD](#sklearn.TruncatedSVD)\n",
    "  - [sklearn.PCA](#sklearn.PCA)\n",
    "\n",
    "# 斷字\n",
    "\n",
    "以下面 `text` 的文字為例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Yeah, I know when I compliment her she won't believe me \\\n",
    "        And it's so sad to think that she don't see what I see \\\n",
    "        But every time she asks me do I look okay? \\\n",
    "        I say When I see your face \\\n",
    "        There's not a thing that I would change \\\n",
    "        Cause you're amazing Just the way you are\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.tokenize\n",
    "\n",
    "- 逗號、問號變成獨立的字\n",
    "- 縮寫 `n't`、`'s` 與 `'re` 被斷成獨立的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from os.path import abspath\n",
    "\n",
    "# prepare `punkt` corpus\n",
    "nltk.download('punkt', download_dir='.nltk_data')\n",
    "nltk.data.path = [abspath('.nltk_data')]\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.CountVectorizer\n",
    "\n",
    "- 移除逗號、問號\n",
    "- 縮寫 `'t` 與 `'s` 被移除；但縮寫 `'re` 被斷成獨立的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "print(CountVectorizer().build_tokenizer()(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras.text_to_word_sequence\n",
    "\n",
    "- 全轉小寫\n",
    "- 移除逗號、問號\n",
    "- 保留縮寫\n",
    "\n",
    "看起來效果最好，不過速度最慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 單字轉向量 / Word to Vector\n",
    "\n",
    "以下面 `words` 的單字串列為例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['cute', 'beauty', 'cold', 'cold', 'cold', 'hot', 'beauty', 'cute']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "word_ids = le.fit_transform(words)\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "word_ids = le.fit_transform(words)\n",
    "one_hot = OneHotEncoder(sparse=False)\n",
    "print(one_hot.fit_transform(word_ids.reshape(len(word_ids), 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras.to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "word_ids = le.fit_transform(words)\n",
    "print(to_categorical(word_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件轉向量 / Document to Vector\n",
    "\n",
    "以下面 `docs` 的文件串列為例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['bobo is cute', \\\n",
    "        'bobo is smart', \\\n",
    "        'bobo is humorous']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "doc_term = cv.fit_transform(docs)\n",
    "print(cv.get_feature_names())\n",
    "print(doc_term.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.TfidfVectorizer\n",
    "\n",
    "- tf: term frequency\n",
    "- idf: inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer()\n",
    "doc_term = tv.fit_transform(docs)\n",
    "print(tv.get_feature_names())\n",
    "print(doc_term.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer()\n",
    "doc_term = tv.fit_transform(docs)\n",
    "svd = TruncatedSVD(n_components=3)\n",
    "print(svd.fit_transform(doc_term.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer()\n",
    "doc_term = tv.fit_transform(docs)\n",
    "pca = PCA(n_components=3, svd_solver='full')\n",
    "print(pca.fit_transform(doc_term.toarray()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
