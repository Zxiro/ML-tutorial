{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy \n",
    "\n",
    "[NumPy](http://www.numpy.org/) is the main package for scientific computing in Python. It is maintained by a large [community](www.numpy.org). In this document you will\n",
    "\n",
    "* Learn how to use numpy.\n",
    "* Implement some basic core deep learning functions such as the softmax, sigmoid, dsigmoid, etc...\n",
    "* Learn how to handle data by normalizing inputs and reshaping images.\n",
    "\n",
    "## 1. Plot figure with numpy \n",
    "\n",
    "In this section you will learn several key numpy functions such as `np.exp()`, `np.log()`, and `np.reshape()`. Before using `np.exp()`, you will use `math.exp()` to implement the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function). You will then see why `np.exp()` is preferable to `math.exp()`.\n",
    "\n",
    "**Exercise**: Build a function that returns the sigmoid of a real number `x`. Use `math.exp(x)` for the exponential function.\n",
    "\n",
    "**Reminder**: $sigmoid(x) = \\frac{1}{1+e^{-x}}$ is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = 5\n",
    "y = 1 / (1 + math.exp(-x))\n",
    "print(y)\n",
    "\n",
    "# Actually, we rarely use the `math` library in deep learning because the inputs of the functions are real numbers. \n",
    "# In deep learning we mostly use matrices and vectors. This is why numpy is more useful.\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.exp(x) # result is (exp(1), exp(2), exp(3))\n",
    "print(y)\n",
    "\n",
    "# So does\n",
    "y = x + 3\n",
    "print(y)\n",
    "\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "print(y)\n",
    "\n",
    "x = np.arange(-10, 10, 0.5)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "plt.plot(x, y, 'r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoid(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. shape\n",
    "\n",
    "In numpy, `shape` is the dimension of a matrix/vector.\n",
    "\n",
    "* `X.shape` is used to get the shape (dimension) of a matrix/vector `X`. See [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) for more information. \n",
    "* `X.reshape(...)` is used to reshape `X` into some other dimension. See [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) for more information. \n",
    "\n",
    "For example, in computer science, an image is represented by a 3D array of shape (width, height, depth=3). However, when you read an image as the input of an algorithm you convert it to a vector of shape (width∗height∗3, 1). In other words, you \"unroll\", or reshape, the 3D array into a 1D vector.\n",
    "\n",
    "![image_numpy_array](http://p8o3egtyk.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week1_week2/2.png)\n",
    "reference: [logistic-regression-with-a-neural-network-mindset](https://snaildove.github.io/2018/02/05/logistic-regression-with-a-neural-network-mindset_week1_and_week2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a 3x3x2 matrix\n",
    "x = np.array([\n",
    "    [\n",
    "        [ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]\n",
    "    ],\n",
    "    [\n",
    "        [ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]\n",
    "    ],\n",
    "    [\n",
    "        [ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]\n",
    "    ]\n",
    "])\n",
    "print(x.shape)\n",
    "\n",
    "reshaped_x = x.reshape((x.shape[0] * x.shape[1] * x.shape[2], 1))\n",
    "print(reshaped_x)\n",
    "\n",
    "# Also we can use -1 to calculate the specific dimension automatically.\n",
    "\n",
    "reshaped_x = x.reshape((-1, 1))\n",
    "print(reshaped_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Row normalization\n",
    "\n",
    "A common technique we use in Machine Learning is to normalize our data. Here, by normalization we mean changing `x` to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of `x` by its norm). For example, if\n",
    "\n",
    "$$\n",
    "x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{1}\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) =\n",
    "\\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{2}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ x\\_normalized = \\frac{x}{\\| x\\|} =\n",
    "\\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}.\\tag{3}$$\n",
    "\n",
    "Note that you can divide matrices of different sizes and it works fine: this is called broadcasting (see the next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 3, 4], [1, 6, 4]])\n",
    "\n",
    "print(np.__version__)\n",
    "x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "x_norm.reshape((-1, 1))\n",
    "print(x_norm.shape)\n",
    "x = x / x_norm\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Broadcasting and the softmax function\n",
    "\n",
    "A very important concept to understand in numpy is \"broadcasting\". It is very useful for performing mathematical operations between arrays of different shapes. For the full details on broadcasting, you can read the official [broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n",
    "\n",
    "**Exercise**: Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes.\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "* $ \\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{, } softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1 && x_2 && ... && x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}} && \\frac{e^{x_2}}{\\sum_{j}e^{x_j}} && ...  && \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "* $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{, $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$\n",
    "\n",
    "$$\n",
    "softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)} \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    ... \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "\n",
    "x_exp = np.exp(x)\n",
    "x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "softmax = x_exp / x_sum\n",
    "\n",
    "print('softmax(x) = ' + str(softmax))\n",
    "\n",
    "# Note: You can print the shape of `x_exp`, `x_sum`. `x_exp`/`x_sum` works due to python broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement the L1 and L2 loss functions\n",
    "\n",
    "The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n",
    "\n",
    "L1 loss is defined as:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{4}$$\n",
    "\n",
    "L2 loss is defined as:\n",
    "$$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "l1_loss = np.sum(np.abs(yhat - y), keepdims=True)\n",
    "print(l1_loss)\n",
    "\n",
    "l2_loss = np.dot((yhat - y), (yhat - y))\n",
    "print(l2_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
