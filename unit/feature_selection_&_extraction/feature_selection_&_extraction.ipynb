{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection & Feature Extraction\n",
    "\n",
    "How much information you provide for model really matters. In Machine Learning, we often have to deal with the [\"Curse of dimensionality\"](https://en.wikipedia.org/wiki/Curse_of_dimensionality) issue. we provide two kinds of algorithm in this tutorial to solve the problem.\n",
    "\n",
    "As you can see in following image, in general case the model which takes data with more dimention should has better performance. However, one can see that as the dimension increasing the performance goes worse.\n",
    "\n",
    "![optimal dimension](http://scikit-learn.org/stable/_images/sphx_glr_plot_rfe_with_cross_validation_001.png)\n",
    "\n",
    "\n",
    "### <span style=\"color:blue\">Feature Selection</span>\n",
    "\n",
    "Following is the quotation from the definition of \"Feature Selection\" on [wikipedia](https://en.wikipedia.org/wiki/Feature_selection).\n",
    "\n",
    ">In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons:\n",
    "\n",
    "> 1. simplification of models to make them easier to interpret by researchers/users,\n",
    "> 2. shorter training times,\n",
    "> 3. to avoid the curse of dimensionality,\n",
    "> 4. enhanced generalization by reducing overfitting (formally, reduction of variance)\n",
    "\n",
    "Based on any **given model** and a **index of performance**. In this tutorial, we use Gaussian Naive Bayess and model's accuracy as example to demostrate just like the assignment. We use feature selection methods to sort features by their relation with model performance.\n",
    "\n",
    "Assume you have a dataset, and each data has 20 features. You are asked to pick the features whose importance are ranked 1 and 2. This method would be very useful either in visulaization or reducing the number of features. With less features, you can not only train your model in less time but measure the robustness of the model.\n",
    "\n",
    "Following are the two common methods to do feature selection: **backward selection** and **forward selection**. You can check the rest of the methods in the reference.\n",
    "\n",
    "---\n",
    "\n",
    "* <span style=\"color:green\">backward selection</span>\n",
    "\n",
    "    As the name suggests, we select the features out backward. we will use our imaginary dataset to demo specific steps of backward selection.\n",
    "    \n",
    "    1. Train the model with all 20 features and test the model for accuracy.\n",
    "           ( fea_01, fea_02, fea_03, fea_04, fea_05 .... fea_20) -> total_acc\n",
    "       \n",
    "    2. Iterate 20 features, and eliminate corresponding feature. Now we train the same model with different combination of 19 features in each iteration.\n",
    "      \n",
    "           ( fea_02, fea_03, fea_04, fea_05, .... fea_20 ) -> acc_01\n",
    "           ( fea_01, fea_03, fea_04, fea_05, .... fea_20 ) -> acc_02\n",
    "           ( fea_01, fea_02, fea_04, fea_05, .... fea_20 ) -> acc_03\n",
    "            ....\n",
    "           ( fea_01, fea_02, fea_03, fea_04, .... fea_19 ) -> acc_20\n",
    "       \n",
    "    3. Eliminate the feature which reduce the accuracy least.\n",
    "    \n",
    "        ```python=\n",
    "        feature_index_we_dont_want = argmin( accuracy )\n",
    "        del features[ feature_index_we_dont_want ]\n",
    "        ```\n",
    "        \n",
    "    4. Loop above 1~3 steps until feature length satisfy the condition.\n",
    "\n",
    "    ---\n",
    "* <span style=\"color:green\">forward selection</span>\n",
    "\n",
    "    Compared with backward selection, we start our selection with empty list. Append the feature list once at a time. Each time we append a feature which contribute to accuracy most.\n",
    "    \n",
    "    1. Leave the feature list empty and calculate accuracy with each feature.\n",
    "            feature_we_want : []\n",
    "            fea_01 -> acc_01, fea_02 -> acc_02, fea_03 -> acc_03 .... fea_20 ->acc_20\n",
    "    2. Append the feature which is the most important.\n",
    "            feature_index_we_want = argmax( accuracy )\n",
    "            feature_ws_want.append( features[ feature_index_we_want ] )\n",
    "    3. Loop above 2 steps until feature length satisfy our condition.\n",
    "    \n",
    "### <span style=\"color:blue\">Feature Extraction</span>\n",
    "\n",
    "According to the definition of feature extraction on [wikipedia](https://en.wikipedia.org/wiki/Feature_extraction):\n",
    ">In machine learning, pattern recognition and in image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is a dimensionality reduction process, where an initial set of raw variables is reduced to more manageable groups (features) for processing, while still accurately and completely describing the original data set.\n",
    "\n",
    "Compared with Feature selection, we create several new feature from given features instead of just filtering the features we need. The most famous feature extraction algorithm is [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis). The tutorial will explain when and why we should use PCA, and leave the complex mathematical theory in reference.\n",
    "\n",
    "In general, feature extraction is more powerful than feature selection with same number of features. However, just like most machine learning model, it lacks explainability. If the purpose is only for visualization in 2D, this kind of algorithm may be the best choice.\n",
    "\n",
    "Sklearn provide a convinient interface to use PCA. Following is a demostration of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "\n",
    "original_x, y = make_blobs(n_samples=500, centers=3, n_features=20)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(x)\n",
    "reduced_x = pca.transform(x)\n",
    "print(\"original shape is \", original_x.shape, \", the shape after extraction is \", reduced_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like Feature selection, there are still lots of other methods you can use. I leave them in reference.\n",
    "\n",
    "---\n",
    "reference\n",
    "* [Other methods available on sklearn](http://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "* [Feature extraction algorithms](https://www.sciencedirect.com/science/article/pii/0031320371900033)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "Fill in the Todo segements of following backward selection implementation.\n",
    "Use backward selection filter out 18 features, and leave only 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "n_features = 20\n",
    "x, y = make_blobs(n_samples=500, centers=3, n_features=n_features)\n",
    "classifier = GaussianNB() # in order to save time and computation power stick to GNB model instead of other model\n",
    "\n",
    "# you should implement the function like Recursive Feature Elimination in sklearn\n",
    "# http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html\n",
    "\n",
    "# external loop to remove feature once at a time.\n",
    "for n_remove_fea in range(0, n_features - 2):\n",
    "    \n",
    "    # internal loop compare the contribution of each feature. \n",
    "    for i in range(0, n_features - n_remove_fea):\n",
    "        # TODO: delete the column you are testing, hint: np.delete\n",
    "    \n",
    "    # TODO: use np.argmin to get the least important feature and delete it\n",
    "\n",
    "# x.shape is going to be (500, 2) now, and backward selection is done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
