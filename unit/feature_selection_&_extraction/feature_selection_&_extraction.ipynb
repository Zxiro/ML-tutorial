{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection & Extraction\n",
    "\n",
    "How much information you provide for model really matters. In Machine Learning, we often have to deal with the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) issue. For example, the following image shows a practical model which delivers worse performance as the dimension increases. Feature selection and extraction are two common methods to solve this issue.\n",
    "\n",
    "![optimal dimension](http://scikit-learn.org/stable/_images/sphx_glr_plot_rfe_with_cross_validation_001.png)\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "According to the [Wikipedia](https://en.wikipedia.org/wiki/Feature_selection):\n",
    "\n",
    "> In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons:\n",
    "\n",
    "> * simplification of models to make them easier to interpret by researchers/users,\n",
    "> * shorter training times,\n",
    "> * to avoid the curse of dimensionality,\n",
    "> * enhanced generalization by reducing overfitting (formally, reduction of variance)\n",
    "\n",
    "For example, if you have a 20D dataset and you want to visualize it in a 2D plot. You can use feature selection to pick two features with the highest importance. Another practical benefit is that, with less features, you can train your model in less time. Following are two common methods for feature selection: **backward selection** and **forward selection**. You can check [other methods available on sklearn](http://scikit-learn.org/stable/modules/feature_selection.html).\n",
    "\n",
    "### Backward selection\n",
    "\n",
    "In backward selection, we starts from all features and remove them one-by-one. Following is a backwrod selection example on a 20D dataset:\n",
    "    \n",
    "1. We create 20 19D-datasets by removing each feature indiviudially and use them to generate 20 models.\n",
    " \n",
    "```\n",
    "( fea_02, fea_03, fea_04, fea_05, .... fea_20 ) -> acc_01\n",
    "( fea_01, fea_03, fea_04, fea_05, .... fea_20 ) -> acc_02\n",
    "  ....\n",
    "( fea_01, fea_02, fea_03, fea_04, .... fea_19 ) -> acc_20\n",
    "```\n",
    "       \n",
    "2. Eliminate the feature corresponding to the model with the highest performance, since the deletion of it is \"least\" important.\n",
    "    \n",
    "```\n",
    "feature_index_we_dont_want = argmax( [acc_01, acc_02, ..., acc_20] )\n",
    "del features[ feature_index_we_dont_want ]\n",
    "```\n",
    "        \n",
    "3. Repeat above steps.\n",
    "\n",
    "\n",
    "### Forward selection\n",
    "\n",
    "In comparison with backward selection, forward selection starts from an empty list and add features one-by-one. Each time we add a feature which contributes to performance most.\n",
    "    \n",
    "1. We create 20 1D-datasets by adding each feature indiviudially and use them to generate 20 models.\n",
    "\n",
    "```\n",
    "( fea_01 ) -> acc_01\n",
    "( fea_02 ) -> acc_02\n",
    "  ....\n",
    "( fea_20 ) -> acc_20\n",
    "```\n",
    "\n",
    "2. Add the feature corresponding to the model with the highest performance, since the addition of it is \"most\" important.\n",
    "\n",
    "```\n",
    "feature_index_we_want = argmmax( [acc_01, acc_02, ..., acc_20] )\n",
    "features_we_want.append( feature_index_we_want )\n",
    "```\n",
    "\n",
    "3. Repeat above steps.\n",
    "    \n",
    "## Feature Extraction\n",
    "\n",
    "According to the [Wikipedia](https://en.wikipedia.org/wiki/Feature_extraction):\n",
    "\n",
    "> In machine learning, pattern recognition and in image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is a dimensionality reduction process, where an initial set of raw variables is reduced to more manageable groups (features) for processing, while still accurately and completely describing the original data set.\n",
    "\n",
    "In comparison with feature selection, feature extraction may create new features of which a new feature could be a linear combination of several existing ones. The most famous feature extraction algorithm is [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis). You may see [other feature extraction algorithms](https://www.sciencedirect.com/science/article/pii/0031320371900033). The code below shows a PCA example with sklearn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "\n",
    "original_x, y = make_blobs(n_samples=500, centers=3, n_features=20)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(x)\n",
    "reduced_x = pca.transform(x)\n",
    "print(\"original shape is \", original_x.shape, \", the shape after extraction is \", reduced_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Fill in the TODO segements in the following code cell, which uses backward selection to filter out 18 features and leave 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "n_features = 20\n",
    "x, y = make_blobs(n_samples=500, centers=3, n_features=n_features)\n",
    "classifier = GaussianNB() # in order to save time and computation power, please fix the model to GNB\n",
    "\n",
    "# you should implement the function like Recursive Feature Elimination in sklearn\n",
    "# http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html\n",
    "\n",
    "# external loop to remove feature once at a time\n",
    "for n_remove_fea in range(0, n_features - 2):\n",
    "    \n",
    "    # internal loop to compare the performance of each feature\n",
    "    for i in range(0, n_features - n_remove_fea):\n",
    "        # TODO: delete the feature you are testing, hint: np.delete\n",
    "    \n",
    "    # TODO: use np.argmax to get the least important feature and delete it\n",
    "\n",
    "# x.shape is (500, 2) now and backward selection is done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
