{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your own classifier\n",
    "\n",
    "\n",
    "## 1. Machine learning lib: scikit-learn\n",
    "### What is scikit-learn?\n",
    "\n",
    "Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python.\n",
    "\n",
    "\n",
    "### Data in scikit-learn\n",
    "\n",
    "Data in scikit-learn, with very few exceptions, is assumed to be stored as a two-dimensional array, of shape \\[n_samples, n_features\\].\n",
    "\n",
    "- **n_samples** : The number of samples: each sample is an item to process (e.g. classify). A sample can be a document, a picture, a sound, a video, a row in database or CSV file, or whatever you can describe with a fixed set of quantitative traits.\n",
    "\n",
    "- **n_features** : The number of features or distinct traits that can be used to describe each item in a quantitative manner. Features are generally real-valued, but may be Boolean or discrete-valued in some cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing\n",
    "### Cat classifier\n",
    "#### A. Normalization: \n",
    "Reference to Andrew Ng's course on coursera.\n",
    "The dataset contains pictures of cat and other stuff. Each X represent a single image, and the label of each image is decribed as follow.\n",
    "- 0 : non-cat\n",
    "- 1 : cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_X = np.load('train_X.npy')\n",
    "train_Y = np.load('train_Y.npy')\n",
    "test_X = np.load('test_X.npy')\n",
    "test_Y = np.load('test_Y.npy')\n",
    "\n",
    "\n",
    "print(\"train_X shape: {}\".format(train_X.shape))\n",
    "print(\"train_Y shape: {}\".format(train_Y.shape))\n",
    "print(\"test_X shape: {}\".format(test_X.shape))\n",
    "print(\"test_Y shape: {}\".format(test_Y.shape))\n",
    "#print(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change the index\n",
    "index = 15\n",
    "plt.imshow(train_X[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test data sets so that images of size (64, 64, 3) are flattened into single \n",
    "# vectors of shape (64 * 64 * 3, 1).\n",
    "num_train = train_X.shape[0]\n",
    "num_test = test_X.shape[0]\n",
    "flatten_train_X = train_X.reshape(num_train,-1)\n",
    "flatten_test_X = test_X.reshape(num_test, -1)\n",
    "\n",
    "print(\"Flatten train_X shape: {}\".format(flatten_train_X.shape))\n",
    "print(\"Flatten test_X shape: {}\".format(flatten_test_X.shape))\n",
    "#print(flatten_train_X[0,:10])\n",
    "\n",
    "# To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the \n",
    "# pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
    "# One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract \n",
    "# the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole \n",
    "# numpy array. \n",
    "#\n",
    "# But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the \n",
    "# dataset by 255 (the maximum value of a pixel channel).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we divide each image by 255, and his operation is called normalization. After dividing the image by 255, the original value of color will be rescaled to 0~1. By normalizing the data, the optimization algorithms will be able to find the optimal more efficiently.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_X = flatten_train_X / 255\n",
    "norm_test_X = flatten_test_X / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(norm_train_X, train_Y)\n",
    "print(clf.score(norm_train_X, train_Y))\n",
    "print(clf.score(norm_test_X, test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 25\n",
    "print(\"Model prediction: {}\".format(clf.predict(norm_test_X[index].reshape(1,-1))))\n",
    "plt.imshow(test_X[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pokémon for Data Mining and Machine Learning\n",
    "The [Pokemon dataset](https://www.kaggle.com/alopez247/pokemon) uses the HP, attack, defense...etc to predict the type of Pokemon. The label is the Type_1 column and the rest columns are treated as features.  \n",
    "  \n",
    "The example below utilized the Python package called Pandas to parse and read CSV-liked input data. The basic usage is listed below.\n",
    "- [Pandas basic usage](https://pandas.pydata.org/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('pokemon_alopez247.csv')\n",
    "print(\"Classes of type_1: {}\".format(df['Type_1'].unique()))\n",
    "print(\"Classes of Body_Style: {}\".format(df['Body_Style'].unique()))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Categorical Features to Numeric Features\n",
    "\n",
    "| **Ordinal Values** | **Nominal Values** |\n",
    "|--------------------|--------------------|\n",
    "| Generation | Color, Type\n",
    "\n",
    "\n",
    "Different preprocessing method deployed to **Color** and **Type_1** columns:\n",
    "\n",
    "1. **Color**:\n",
    "    * Here, we're trying to demonstrate one-hot encoding, which transform the numeric features to combinations of 0s and 1. Using numeric features to represent category will confuse the classifier since 0 and 1 are closer than 0 and 2. But Green and Red is not closer than Green and Black.  \n",
    "  \n",
    "2. **Type_1**:\n",
    "    * We didn't transform Type_1 as one-hot due to the fact that Type_1 would be used as label. The classifier we presented in this example is SVM, and SVM takes 0, 1, 2, 3... as different categories. As a result we only transform the string categories to numbers.\n",
    "    * Here, we merged similar types into the same categories as presented below.\n",
    "    \n",
    "|  Type_1  |  catogory  |\n",
    "|--------|---------|\n",
    "|Grass|1|\n",
    "|Fire|2|\n",
    "|Water+Ice|3|\n",
    "|Bug|4|\n",
    "|Normal|5|\n",
    "|Poison + Ghost + Dark|6|\n",
    "|Electric|7|\n",
    "|Ground + Rock|8|\n",
    "|Flying + Fairy + Dragon|9|\n",
    "|Fighting + Psychic + Steel|10|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dictionary = {\"Type_1\":{ 'Grass': 1, 'Fire': 2, 'Water': 3, 'Bug': 4, 'Normal': 5, 'Poison': 6, 'Electric': 7, 'Ground': 8, 'Fairy': 9, \n",
    " 'Fighting': 10, 'Psychic': 10, 'Rock': 8, 'Ghost': 6, 'Ice': 3, 'Dragon': 9, 'Dark':6, 'Steel': 10, 'Flying': 9}}\n",
    "df = df.replace(mapping_dictionary) # 透過 replace function 可以方便地把字串改為對應的數字\n",
    "df[\"isLegendary\"] = df[\"isLegendary\"].astype(int) # Boolean to int\n",
    "df[\"hasMegaEvolution\"] = df[\"hasMegaEvolution\"].astype(int)\n",
    "\n",
    "dummy_df = pd.get_dummies(df['Color'])  ## one-hot encoding\n",
    "df = pd.concat([df, dummy_df], axis=1)\n",
    "df = df.drop('Color', axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe() # description of the dataset only for numeric feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop column because it if irrelevant to the results\n",
    "   \n",
    "df = df.drop(['Number','Name', 'Type_2', 'Egg_Group_1', 'Egg_Group_2', 'hasGender', 'Body_Style'],axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data shape after drop column\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes count of type_1\n",
    "# mind unbalanced data distribution\n",
    "df['Type_1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing value\n",
    "\n",
    "There are many methods to deal with missing value such as dorpping the feature/sample, or giving values by zero/column mean/interpolation. There is not always right way to do, it depends upon your domain knowledge or experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Missing value\n",
    "print(df.isnull().sum()) # Pr_Male has 77 missing value\n",
    "df.dropna(axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need all features are numeric, check if there still has dtype = object\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally check whether all features are done by the steps we described above.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Also you can draw histgram using pandas\n",
    "df[['HP', 'Attack']].plot.hist(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "def get_arrays(df):\n",
    "    X = np.array(df.iloc[:,1:])\n",
    "    y = np.array(df['Type_1'])\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = get_arrays(df_train)\n",
    "test_X, test_Y = get_arrays(df_test)\n",
    "scaler = StandardScaler()\n",
    "svc = SVC(C=5, gamma=0.04)\n",
    "clf = Pipeline([('scaler', scaler), ('svc', svc)])\n",
    "clf.fit(train_X, train_Y)\n",
    "print(\"Accuracy: {}\".format(clf.score(train_X, train_Y)))\n",
    "print(\"Accuracy: {}\".format(clf.score(test_X, test_Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建構 [UCI German Credit Data](https://onlinecourses.science.psu.edu/stat857/node/215) 分類模型\n",
    "\n",
    " * 該資料集包含 1000 筆貸款資料，其中 700 筆正樣本(credit-worthy)以及 300 筆負樣本(not credit-worthy)。每個樣本有 20 個特徵，其中 17 個類別(categorical)特徵，3 個為數值(numeric)特徵，請參考[特徵的細節](https://onlinecourses.science.psu.edu/stat857/node/222)。\n",
    " * 利用準備好的工具，建構分類模型，請參考 [Classifier comparison](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) 選擇機器學習分類器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入準備好的工具\n",
    "import sys\n",
    "sys.path.append('.prepared')\n",
    "import classification as prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料集\n",
    "\n",
    "UCI German Credit Data 包含 800 筆訓練資料與 200 筆測試資料。每筆由一個 20 維的特徵向量與一個 `1` 或 `0` 的類別組成。其中 `1` 代表正樣本(credit-worthy)；`0` 代表負樣本(not credit-worthy)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prepared.x_train.shape) # 800 筆訓練資料的特徵\n",
    "print(prepared.x_train[:3])   # 印出前三筆訓練資料的特徵\n",
    "print(prepared.y_train.shape) # 800 筆訓練資料的類別\n",
    "print(prepared.y_train[:3])   # 印出前三筆訓練資料的類別\n",
    "print(prepared.x_test.shape)  # 200 筆訓練資料的特徵\n",
    "print(prepared.y_test.shape)  # 200 筆訓練資料的類別"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示範分類器\n",
    "\n",
    "使用訓練資料(`x_train` 與 `y_train`)訓練示範分類器(`demo_clf`)，在訓練資料與測試資料(`x_test` 與 `y_test`)上評估其正確率。請修改下方[動手做](#動手做)的程式碼試著超越這個示範分類器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用訓練資料訓練示範分類器, `demo_clf`\n",
    "demo_clf = prepared.demo(prepared.x_train, prepared.y_train)\n",
    "\n",
    "# 在訓練資料與測試資料上評估其正確率\n",
    "prepared.evaluate(demo_clf, prepared.x_train, prepared.x_test, prepared.y_train, prepared.y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 動手做\n",
    "\n",
    "修改以下程式碼，試著使用不同的機器學習演算法，建構比示範分類器更好的模型。換句話說，在測試資料上的正確率超過 `0.765`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import classifiers you want to use\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO: try different classifiers\n",
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "clf = clf.fit(prepared.x_train, prepared.y_train) # train `clf`\n",
    "prepared.evaluate(clf, prepared.x_train, prepared.x_test, prepared.y_train, prepared.y_test) # evaluate `clf`\n",
    "\n",
    "if 'DecisionTreeClassifier' == type(clf).__name__: # if `clf` is a decision tree\n",
    "    prepared.plot(clf) # plot the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference to  Andrew Ng, Professor Lin 's course on coursera and Professor Wu in NTHU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
