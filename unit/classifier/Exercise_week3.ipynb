{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is scikit-learn?\n",
    "\n",
    "Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python.\n",
    "\n",
    "\n",
    "## Data in scikit-learn\n",
    "\n",
    "Data in scikit-learn, with very few exceptions, is assumed to be stored as a two-dimensional array, of shape \\[n_samples, n_features\\].\n",
    "\n",
    "- **n_samples** : The number of samples: each sample is an item to process (e.g. classify). A sample can be a document, a picture, a sound, a video, a row in database or CSV file, or whatever you can describe with a fixed set of quantitative traits.\n",
    "\n",
    "- **n_features** : The number of features or distinct traits that can be used to describe each item in a quantitative manner. Features are generally real-valued, but may be Boolean or discrete-valued in some cases.\n",
    "\n",
    "\n",
    "\n",
    "## General Machine Learning Steps\n",
    "\n",
    "1. Data collection, preprocessing (e.g., integration, cleaning, etc.), and exploration:\n",
    "    - Split a dataset into the training and testing datasets\n",
    "2. Model development:\n",
    "    - Assume a model $\\mathcal F: \\{f_1, f_2, \\cdots \\}$ that is a collection of candidate functions  $\\mathcal f$ Let's assume that each  $\\mathcal f$ is parametrized by $\\mathcal w$.\n",
    "    - Define a cost function $\\mathcal C(w)$ that measures \"how good a particular $\\mathcal f$ can explain the training data\". The lower the cost function the better.\n",
    "3. Training: employ an algorithm that finds the best (or good enough) function $\\mathcal f^∗$ in the model that minimizes the cost function over the training dataset\n",
    "4. Testing: evaluate the performance of the learned $\\mathcal f^∗$ using the testing dataset.\n",
    "5. Apply the model in the real world.\n",
    "\n",
    "> The data is presented to the algorithm usually as a two-dimensional array (or matrix) of numbers. Each data point (also known as a sample or training instance) that we want to either learn from or make a decision on is represented as a list of numbers, a so-called feature vector, and its containing features represent the properties of this point.\n",
    "\n",
    "> In classification, the label is discrete, such as \"spam\" or \"no spam\". In other words, it provides a clear-cut distinction between categories. Furthermore, it is important to note that class labels are nominal, not ordinal variables. Nominal and ordinal variables are both subcategories of categorical variable. Ordinal variables imply an order, for example, T-shirt sizes \"XL > L > M > S\". On the contrary, nominal variables don't imply an order, for example, we (usually) can't assume \"orange > blue > green\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cat classifier - Reference to Andrew Ng's course on coursera.\n",
    "The dataset contains pictures of cat and other stuff. Each X represent a single image, and the label of each image is decribed as follow.\n",
    "- 0 : non-cat\n",
    "- 1 : cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_X = np.load('train_X.npy')\n",
    "train_Y = np.load('train_Y.npy')\n",
    "test_X = np.load('test_X.npy')\n",
    "test_Y = np.load('test_Y.npy')\n",
    "\n",
    "\n",
    "print(\"train_X shape: {}\".format(train_X.shape))\n",
    "print(\"train_Y shape: {}\".format(train_Y.shape))\n",
    "print(\"test_X shape: {}\".format(test_X.shape))\n",
    "print(\"test_Y shape: {}\".format(test_Y.shape))\n",
    "#print(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change the index\n",
    "index = 15\n",
    "plt.imshow(train_X[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test data sets so that images of size (64, 64, 3) are flattened into single \n",
    "# vectors of shape (64 * 64 * 3, 1).\n",
    "num_train = train_X.shape[0]\n",
    "num_test = test_X.shape[0]\n",
    "flatten_train_X = train_X.reshape(num_train,-1)\n",
    "flatten_test_X = test_X.reshape(num_test, -1)\n",
    "\n",
    "print(\"Flatten train_X shape: {}\".format(flatten_train_X.shape))\n",
    "print(\"Flatten test_X shape: {}\".format(flatten_test_X.shape))\n",
    "#print(flatten_train_X[0,:10])\n",
    "\n",
    "# To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the \n",
    "# pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
    "# One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract \n",
    "# the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole \n",
    "# numpy array. \n",
    "#\n",
    "# But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the \n",
    "# dataset by 255 (the maximum value of a pixel channel).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we divide each image by 255, and his operation is called normalization. After dividing the image by 255, the original value of color will be rescaled to 0~1. By normalizing the data, the optimization algorithms will be able to find the optimal more efficiently.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_X = flatten_train_X / 255\n",
    "norm_test_X = flatten_test_X / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(norm_train_X, train_Y)\n",
    "print(clf.score(norm_train_X, train_Y))\n",
    "print(clf.score(norm_test_X, test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 25\n",
    "print(\"Model prediction: {}\".format(clf.predict(norm_test_X[index].reshape(1,-1))))\n",
    "plt.imshow(test_X[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [Kaggle - Pokémon for Data Mining and Machine Learning](https://www.kaggle.com/alopez247/pokemon)\n",
    "The Pokemon dataset uses the HP, attack, defense...etc to predict the type of Pokemon. The label is the Type_1 column and the rest columns are treated as features.  \n",
    "  \n",
    "The example below utilized the Python package called Pandas to parse and read CSV-liked input data. The basic usage is listed below.\n",
    "- [Pandas basic usage](https://pandas.pydata.org/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('pokemon_alopez247.csv')\n",
    "print(\"Classes of type_1: {}\".format(df['Type_1'].unique()))\n",
    "print(\"Classes of Body_Style: {}\".format(df['Body_Style'].unique()))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Categorical Features to Numeric Features\n",
    "\n",
    "- **Ordinal Values**\n",
    "> Generation\n",
    "- **Nominal Values**\n",
    "> Color, Type\n",
    "\n",
    "Different preprocessing method deployed to **Color** and **Type_1** columns:\n",
    "\n",
    "1. **Color**:\n",
    "    * Here, we're trying to demonstrate one-hot encoding, which transform the numeric features to combinations of 0s and 1. Using numeric features to represent category will confuse the classifier since 0 and 1 are closer than 0 and 2. But Green and Red is not closer than Green and Black.  \n",
    "  \n",
    "2. **Type_1**:\n",
    "    * We didn't transform Type_1 as one-hot due to the fact that Type_1 would be used as label. The classifier we presented in this example is SVM, and SVM takes 0, 1, 2, 3... as different categories. As a result we only transform the string categories to numbers.\n",
    "    * Here, we merged similar types into the same categories as presented below.\n",
    "    \n",
    "|  Type_1  |  catogory  |\n",
    "|--------|---------|\n",
    "|Grass|1|\n",
    "|Fire|2|\n",
    "|Water+Ice|3|\n",
    "|Bug|4|\n",
    "|Normal|5|\n",
    "|Poison + Ghost + Dark|6|\n",
    "|Electric|7|\n",
    "|Ground + Rock|8|\n",
    "|Flying + Fairy + Dragon|9|\n",
    "|Fighting + Psychic + Steel|10|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dictionary = {\"Type_1\":{ 'Grass': 1, 'Fire': 2, 'Water': 3, 'Bug': 4, 'Normal': 5, 'Poison': 6, 'Electric': 7, 'Ground': 8, 'Fairy': 9, \n",
    " 'Fighting': 10, 'Psychic': 10, 'Rock': 8, 'Ghost': 6, 'Ice': 3, 'Dragon': 9, 'Dark':6, 'Steel': 10, 'Flying': 9}}\n",
    "df = df.replace(mapping_dictionary) # 透過 replace function 可以方便地把字串改為對應的數字\n",
    "df[\"isLegendary\"] = df[\"isLegendary\"].astype(int) # Boolean to int\n",
    "df[\"hasMegaEvolution\"] = df[\"hasMegaEvolution\"].astype(int)\n",
    "\n",
    "dummy_df = pd.get_dummies(df['Color'])  ## one-hot encoding\n",
    "df = pd.concat([df, dummy_df], axis=1)\n",
    "df = df.drop('Color', axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe() # description of the dataset only for numeric feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop column because it if irrelevant to the results\n",
    "   \n",
    "df = df.drop(['Number','Name', 'Type_2', 'Egg_Group_1', 'Egg_Group_2', 'hasGender', 'Body_Style'],axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data shape after drop column\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes count of type_1\n",
    "# mind unbalanced data distribution\n",
    "df['Type_1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value\n",
    "\n",
    "There are many methods to deal with missing value such as dorpping the feature/sample, or giving values by zero/column mean/interpolation. There is not always right way to do, it depends upon your domain knowledge or experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Missing value\n",
    "print(df.isnull().sum()) # Pr_Male has 77 missing value\n",
    "df.dropna(axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need all features are numeric, check if there still has dtype = object\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally check whether all features are done by the steps we described above.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Also you can draw histgram using pandas\n",
    "df[['HP', 'Attack']].plot.hist(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "def get_arrays(df):\n",
    "    X = np.array(df.iloc[:,1:])\n",
    "    y = np.array(df['Type_1'])\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = get_arrays(df_train)\n",
    "test_X, test_Y = get_arrays(df_test)\n",
    "scaler = StandardScaler()\n",
    "svc = SVC(C=5, gamma=0.04)\n",
    "clf = Pipeline([('scaler', scaler), ('svc', svc)])\n",
    "clf.fit(train_X, train_Y)\n",
    "print(\"Accuracy: {}\".format(clf.score(train_X, train_Y)))\n",
    "print(\"Accuracy: {}\".format(clf.score(test_X, test_Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Build a classifier on stock data (predict a stock will rise(1) or not(0))\n",
    "\n",
    "- Please refer to week03_classifier/stock_system.ipynb for detailed illustraion.\n",
    "- Load the data in the stock directory.\n",
    "- Choose a classifier in sklearn package(SVC, decision tree, KNN, MLP, etc.)\n",
    "- Make the predictions on test data and report the results.\n",
    "- Raw data is in \"/home/mlb/res/stock/twse/raw/\"  or json data is in \"/home/mlb/res/stock/twse/json/\"\n",
    "- Feature set consists of thirty features(six features each day, high price 高點, low price 低點, open price 開盤價, close price 收盤價, adjust_close 最高最低價, volume 成交量)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note:\n",
    "# We already parse raw data and save it to npy format\n",
    "# for you.This exercise is only for you to build a \n",
    "# model conveniently. Please parse raw data and preprocess\n",
    "# it for your own model in stock simulation.\n",
    "\n",
    "stock_train_X = np.load('stock/train_X.npy') # train 2017-05-01 ~ 2017-05-31\n",
    "stock_train_Y = np.load('stock/train_Y.npy')\n",
    "stock_test_X = np.load('stock/test_X.npy') # test 2017-06-01 ~ 2017-06-30\n",
    "stock_test_Y = np.load('stock/test_Y.npy')\n",
    "\n",
    "# ... build your own classifier with module in sklearn or other lib\n",
    "\n",
    "print(stock_train_Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference to  Andrew Ng, Professor Lin 's course on coursera and Professor Wu in NTHU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
