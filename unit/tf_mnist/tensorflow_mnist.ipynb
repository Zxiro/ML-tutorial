{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 簡易教學與實作\n",
    "\n",
    "Tensorflow 是由 Google 所發布的深度學習套件，讓開發者可以自行設計神經網路的架構。其優點在於在設計神經網路上非常靈活，相較於其他深度學習套件，開發者對於神經網路的運作，有較全面的掌控，而缺點則是入門門檻較高。\n",
    "\n",
    "## 1. 兩種 tensor\n",
    "\n",
    "Tensor 在數學上的意義是任意維度的矩陣，從數學來看，**神經網路就是由許多的 tensor 所組成**。考慮以下一層神經網路的數學式：\n",
    "\n",
    "$y = activation(x \\times weight + bias)$\n",
    "\n",
    "在建構神經網路模型的過程中，必須不斷從資料中取出一部分的 `X` 跟 `Y` 來訓練神經網路，更新 `Weight` 跟 `Bias`。從訓練模型的角度來說，`X` 跟 `Y` 是輸入，而 `Weight` 與 `Bias` 是輸出。為了因應上述的特性，Tensorflow 提供了兩種不同特性的 tensor：\n",
    "\n",
    "1. tensorflow.placeholder()：其值從外部輸入，對應 `X` 與 `Y`。\n",
    "2. tensorflow.Variable()：其值在訓練過程自動更新，對應 `Weight` 與 `Bias`。\n",
    "\n",
    "接下來會介紹這兩種 tensor 的用法，在介紹之前，先定義兩個名詞：\n",
    "\n",
    "1. tensor：任意維度的矩陣。\n",
    "2. shape：tensor 在每一維度上的大小。\n",
    "\n",
    "### 1.1 tensorflow.placeholder()\n",
    "\n",
    "這種 tensor 用來存放外部輸入的資料，在訓練時過程中不會被更新，用法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.placeholder(tf.float32, shape = (3, 2))\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如此產生了一個 shape 為 (3, 2) 但是沒有任何數值內容的 tensor，而給值的方法會在下一小節探討。在實務上，每次輸入的資料筆數不一定相同，因此 `tf.placeholder()` 也可以產生連 shape 都不明確的 tensor："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "b = tf.placeholder(tf.float32, shape = (None, 2))\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`b` 之後可以放入 shape 為 `(n, 2)` 的矩陣，這裡 `n` 就是一次訓練所使用的資料筆數。\n",
    "\n",
    "### 1.2 tensorflow.Variable()\n",
    "\n",
    "這種 tensor 的特性是在神經網路訓練過程中，其值會自動被更新。實務上會用這種 tensor 來存放神經網路的 `Weight` 跟 `Bias`，其具體用法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3,) dtype=int32_ref>\n",
      "<tf.Variable 'Variable_1:0' shape=(3, 2) dtype=int32_ref>\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable([1, 2, 3])\n",
    "b = tf.Variable([[2, 4], [6, 8], [10, 12]])\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的程式碼產生了一個 1 維 tensor 與一個 2 維 tensor，其中初始值各為：\n",
    "\n",
    "$\n",
    "a = \n",
    "\\begin{bmatrix}\n",
    "    1 & 2 & 3\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "b = \n",
    "\\begin{bmatrix}\n",
    "    2 & 4 \\\\\n",
    "    6 & 8 \\\\\n",
    "    10 & 12\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般神經網路中 `Weight` 與 `Bias` 的初始值是隨機指定，`tf.Variable()` 也提供這樣的初始化方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_2:0' shape=(3, 2) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "c = tf.Variable(tf.random_uniform(shape = (3, 2)))\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 獲取 tensor 的值\n",
    "\n",
    "從上面的例子可以注意到 tensor 的值沒辦法透過 print() 來查看，那是因為在 Tensorflow 的架構中，tensor 的值要等到訓練開始之後才會填入，這節將介紹如何訓練神經網路，並獲取其中 tensor 的值。\n",
    "\n",
    "### 3.1 tensorflow.Session()\n",
    "\n",
    "執行一個神經網路。將整個網路的最後一個 tensor 傳給 `tf.Session()`，它會自動執行這個 tensor 以前的所有網路，並回傳執行後這個 tensor 的值。具體用法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[5 6]\n",
      " [7 8]]\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "[[ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.int32, shape = (2, 2))\n",
    "b = tf.placeholder(tf.int32, shape = (None, 2))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "a_value_1 = sess.run(a, feed_dict = {a: [[1,2],[3,4]]})\n",
    "a_value_2 = sess.run(a, feed_dict = {a: [[5,6],[7,8]]})\n",
    "\n",
    "# shape = (2, 2)\n",
    "b_value_1 = sess.run(b, feed_dict = {b: [[1,2],[3,4]]})\n",
    "\n",
    "# shape = (3, 2)\n",
    "b_value_2 = sess.run(b, feed_dict = {b: [[5,6],[7,8], [9, 10]]})\n",
    "\n",
    "print(a_value_1)\n",
    "print(a_value_2)\n",
    "\n",
    "print(b_value_1)\n",
    "print(b_value_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從上面程式碼可以看到，`tf.placeholder()` 透過 `sess.run()` 中的 feed_dict 的參數給值。注意由於 `b` 一開始 shape 為 `(None, 2)`，所以可以給它 (2, 2) 或 (3, 2) 等不同 shape 的矩陣。接著是查看 `tf.Variable()` 的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[[ 0.62298906  0.20584846]\n",
      " [ 0.65348887  0.70709825]\n",
      " [ 0.7138288   0.15067267]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable([1, 2, 3])\n",
    "b = tf.Variable(tf.random_uniform(shape = (3, 2)))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "a_value = sess.run(a)\n",
    "b_value = sess.run(b)\n",
    "\n",
    "print(a_value)\n",
    "print(b_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以注意到由於 `b` 是使用隨機初始的方式，所以值為亂數。\n",
    "\n",
    "## 3. 矩陣運算\n",
    "\n",
    "### 3.1 線性計算\n",
    "\n",
    "神經網路每層在經過激活(activate)之前會有一個矩陣運算：\n",
    "\n",
    "$y = x \\times weight + bias$\n",
    "\n",
    "這條數學式中用到了矩陣乘法跟矩陣加法，而 Tensorflow 中可以使用 `tf.matmul()` 做矩陣乘法，使用 + 做矩陣加法，具體程式碼如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 23 -26]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.int32, shape = (1, 3))\n",
    "weight = tf.Variable([[1, -2], [3, -4], [5, -6]])\n",
    "bias = tf.Variable([1, 2])\n",
    "\n",
    "y = tf.matmul(x, weight) + bias\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(sess.run(y, feed_dict = {x: [[1, 2, 3]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的程式碼對應的數學式如下：\n",
    "\n",
    "$\n",
    "y = \n",
    "\\begin{bmatrix}\n",
    "    1 & 2 & 3\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "    1 & -2 \\\\\n",
    "    3 & -4 \\\\\n",
    "    5 & -6\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "    1 & 2\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "計算出來 `y` 為 $[23, -26]$。\n",
    "\n",
    "### 3.2 激活函數\n",
    "\n",
    "神經網路每層線性的矩陣運算後，還會經過一個激活函數。Tensorflow 提供許多激活函數供開發者使用。具體程式碼如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23  0]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.int32, shape = (1, 3))\n",
    "weight = tf.Variable([[1, -2], [3, -4], [5, -6]])\n",
    "bias = tf.Variable([1, 2])\n",
    "\n",
    "y = tf.matmul(x, weight) + bias\n",
    "\n",
    "# 使用 Relu 激活函數\n",
    "activate = tf.nn.relu(y)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(sess.run(activate, feed_dict = {x: [[1, 2, 3]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的程式碼使用了 `tf.nn.relu()` 來做為激活函數。輸入值小於 0 時，relu 會輸出 0；輸入值大於 0 時，relu 會輸出輸入值。所以將 relu 將[23, -26] 變成 [23, 0]。試著將 `tf.nn.relu()` 替換成另外兩個常見的激活函數，並觀察輸出：\n",
    "\n",
    "1. `tf.nn.sigmoid()`\n",
    "2. `tf.nn.tanh()`\n",
    "\n",
    "有了以上觀念後，可以用 Tensorflow，試著接出以下的數學式：\n",
    "\n",
    "$$\n",
    "x = \n",
    "\\begin{bmatrix}\n",
    "    1 & 2 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h = x \\times\n",
    "\\begin{bmatrix}\n",
    "    1 & -2 \\\\\n",
    "    3 & -4 \\\\\n",
    "    5 & -6\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "    1 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "activate = relu(h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = activate \\times\n",
    "\\begin{bmatrix}\n",
    "    5 & 7 \\\\\n",
    "    6 & 8 \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "    3 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "以下程式區塊可供使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 神經網路\n",
    "\n",
    "上一小節說明了如何用 Tensorflow 建構神經網路，接著要利用 MNIST 資料集來訓練一個可以辨識手寫數字的神經網路。\n",
    "\n",
    "### 4.1 MNIST 資料集 \n",
    "\n",
    "[MNIST 資料集](http://yann.lecun.com/exdb/mnist/) 裡面總共有 70000 張手寫數字圖片，每一張皆為 28 pixels x 28 pixels 的黑白圖片。在 Tensorflow 中取得 MNIST 資料集的程式碼如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-8e55126b2378>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "\n",
    "# 從訓練資料中隨機拿取 100 張圖片以及其對應的標籤\n",
    "x_batch, y_batch = mnist.train.next_batch(100)\n",
    "\n",
    "# 取得所有測試用資料的照片\n",
    "full_test_data_images = mnist.test.images\n",
    "\n",
    "# 取得所有測試用資料的標籤\n",
    "full_test_data_labels = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看看 MNIST 資料集的內容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f19e03f6a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "trainimg = mnist.train.images\n",
    "trainlabel = mnist.train.labels\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(5):\n",
    "    curr_img   = np.reshape(trainimg[i, :], (28, 28))\n",
    "    curr_label = np.argmax(trainlabel[i, :] )\n",
    "    \n",
    "    fig.add_subplot(1, 5, i + 1)\n",
    "    plt.imshow(curr_img, cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "來看看資料的 shape："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = mnist.train.next_batch(100)\n",
    "\n",
    "print(x_batch.shape)\n",
    "print(y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們一次只拿出 100 筆資料，所以 shape 的第一個維度為 100。每張圖片的有 28 x 28 = 784 個像素值，所以 `x_batch` 的第二個維度為 784。標籤可能為 0~9 共 10 個類別，經過 one-hot 編碼後會是 10 維，所以 `y_batch` 的第二個維度為 10。\n",
    "\n",
    "### 4.2 訓練神經網路\n",
    "\n",
    "以下為訓練神經網路的程式碼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-f516495cb542>:17: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Step 100 training loss 2.306363\n",
      "Step 200 training loss 2.311510\n",
      "Step 300 training loss 2.307052\n",
      "Step 400 training loss 2.315013\n",
      "Step 500 training loss 2.305404\n",
      "Step 600 training loss 2.306050\n",
      "Step 700 training loss 2.308618\n",
      "Step 800 training loss 2.317197\n",
      "Step 900 training loss 2.304279\n",
      "Step 1000 training loss 2.275578\n",
      "Step 1100 training loss 2.247457\n",
      "Step 1200 training loss 2.156875\n",
      "Step 1300 training loss 2.049428\n",
      "Step 1400 training loss 1.916977\n",
      "Step 1500 training loss 1.796522\n",
      "Step 1600 training loss 1.687458\n",
      "Step 1700 training loss 1.591968\n",
      "Step 1800 training loss 1.500966\n",
      "Step 1900 training loss 1.407697\n",
      "Step 2000 training loss 1.281204\n",
      "=================\n",
      "Test accuracy: 0.693800\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape = (None, 784))\n",
    "y = tf.placeholder(tf.float32, shape = (None, 10))\n",
    "\n",
    "# 神經網路第一層\n",
    "layer_1_weight = tf.Variable(tf.random_uniform(shape = (784, 100)))\n",
    "layer_1_bias = tf.Variable(tf.random_uniform(shape = (100, )))\n",
    "\n",
    "layer_1_output = tf.nn.sigmoid(tf.matmul(x, layer_1_weight) + layer_1_bias)\n",
    "\n",
    "# 神經網路第二層\n",
    "layer_2_weight = tf.Variable(tf.random_uniform(shape = (100, 10)))\n",
    "layer_2_bias = tf.Variable(tf.random_uniform(shape = (10, )))\n",
    "\n",
    "layer_2_output = tf.matmul(layer_1_output, layer_2_weight) + layer_2_bias\n",
    "\n",
    "# 定義 loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = layer_2_output))\n",
    "\n",
    "# 定義 accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(layer_2_output, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 定義 optimizer 以及 learning rate\n",
    "train_step = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 訓練 2000 次\n",
    "for i in range(1, 2001):\n",
    "    \n",
    "    # 每次訓練隨機選 100 筆資料放進神經網路\n",
    "    x_batch, y_batch = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict = {x: x_batch, y: y_batch})\n",
    "    \n",
    "    # 每訓練 100 次查看一次目前的 loss\n",
    "    if i % 100 == 0:\n",
    "        print('Step %d training loss %.6f' % (i, sess.run(loss, feed_dict = {x: mnist.train.images, y: mnist.train.labels})))\n",
    "        \n",
    "# 查看訓練完後的神經網路的準確率\n",
    "print('=================')\n",
    "print('Test accuracy: %.6f' % sess.run(accuracy, feed_dict = {x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到以上的神經網路最後的準確率大約在 0.7 到 0.8，然而神經網路在 MNIST 上的準確率是可以到達 0.95 以上的，所以接下來請隨意修改上面的程式碼，使其準確率可以超過 0.9\n",
    "\n",
    "提示：\n",
    "\n",
    "- 增加神經網路的層數\n",
    "- 增加神經網路每層的神經元數量\n",
    "- 換一個激活函數\n",
    "- 調整 learning rate\n",
    "- 訓練久一點"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
