{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "###### tags: `mlb`\n",
    "\n",
    "## 1. Time Series Features\n",
    "\n",
    "Consider a sentence as follows : \n",
    "\n",
    "*I am darby. I am handsome.*\n",
    "\n",
    "How do we extract the features from this sentence ?\n",
    "\n",
    "\n",
    "### 1.1 Sentence-level Features\n",
    "- Extract features from each sentence : \n",
    "    - Bag-of-words\n",
    "\n",
    "1. Build a list contain all words of the sentence : \n",
    "[ \"I\", \"am\", \"darby\", \"handsome\" ]\n",
    "\n",
    "2. Generate a vector which lenght is the same as lenght of the list :\n",
    "[ 0, 0, 0, 0 ]\n",
    "\n",
    "3. Replace zero with word frequency : \n",
    "[ 2, 2, 1, 1 ]\n",
    "\n",
    "- Advantage : \n",
    "    - It can fit the most of machine learning model.\n",
    "\n",
    "- Disadvantage : \n",
    "    - Loss the information of relation between words.\n",
    "\n",
    "### 1.2 Word-level Features\n",
    "- Extract features from each word\n",
    "    - One-hot\n",
    "    - Word embedding\n",
    "\n",
    "- Word-level features will contain 2 domains : \n",
    "    - Time domain and word domain.\n",
    "    - For example, we can get the features as follows by one-hot encoding : \n",
    "\n",
    "[[1, 0, 0, 0], => I \n",
    "[0, 1, 0, 0],  => am\n",
    "[0, 0, 1, 0],  => darby\n",
    "[1, 0, 0, 0],  => I\n",
    "[0, 1, 0, 0],  => am\n",
    "[0, 0, 0, 1]]  => handsome\n",
    "\n",
    "- We can flat the features to a vector and feed it to maching learning model.\n",
    "    - But this **cannot** let the model learn the relation between words.\n",
    "\n",
    "## 2. Recurrent Neural Networks\n",
    "\n",
    "### 2.1 Basic Architecture\n",
    "\n",
    "Idea : feed a word and a previous state to machine learning model each time.\n",
    "\n",
    "The model has two output : one for prediction and another for **hidden state**.\n",
    "\n",
    "![](https://i.imgur.com/9BWm6wW.png)\n",
    "\n",
    "If we want to classify a sentence, we can treat the last output as prediction.\n",
    "- Ignore intermediate output.\n",
    "\n",
    "### 2.2 Backpropagation Through Time\n",
    "\n",
    "In this architecture, backpropagation is not enough.\n",
    "- Because we cannot optimize intermediate output.\n",
    "\n",
    "Solution : recurrent neural network is a cyclic graph. Maybe we can simplify the architecture of recurrent neural network.\n",
    "\n",
    "![](https://i.imgur.com/eEAcpJW.png)\n",
    "\n",
    "Here, Model 1, Model 2, ..., Model n share the same weights and bias.\n",
    "\n",
    "We can treat those models as a large, sequential model.\n",
    "- $h_n$ is the output of this model.\n",
    "- Ignore $h_1, h_2, ..., h_{n-1}$\n",
    "\n",
    "Now, we can use backpropagation calculate the gradients easily.\n",
    "\n",
    "Work flow of backpropagation through time : \n",
    "1. Unroll the recurrent neural network.\n",
    "2. Calculate the gradients of all trainable variables.\n",
    "3. Roll-up the recurrent neural network.\n",
    "4. Update trainable variables.\n",
    "\n",
    "## 3. Long Short-Term Memory (LSTM)\n",
    "\n",
    "### 3.1 Introduction\n",
    "\n",
    "Reference : [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "\n",
    "#### Disadvantages of recurrent neural networks : \n",
    "1. Gradient vanish\n",
    "2. Gradient explode\n",
    "3. Performance will be bad if time series is too long.\n",
    "\n",
    "#### Solution : \n",
    "1. An additional new state : **cell state**\n",
    "2. Three gates : **input gate**, **forget gate**, **output gate**\n",
    "\n",
    "#### Simple work flow : \n",
    "\n",
    "Let \n",
    "(1) $x_t$ is a word at time stamp $t$, \n",
    "(2) $h_t$ is the hidden state at time stamp $t$, \n",
    "(3) $C_t$ is the cell state at time stamp $t$.\n",
    "\n",
    "Consider the current word and two previous states : $x_t, h_{t-1}, C_{t-1}$. \n",
    "A LSTM cell calculates three values : $i_t, f_t, o_t$\n",
    "Use those three values to generate next two states : $h_t, C_t$\n",
    "\n",
    "#### Calculate $i_t$ (output of input gate) : \n",
    "\n",
    "$$\n",
    "i_t = Sigmoid(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "\n",
    "#### Calculate $f_t$ (output of forget gate) : \n",
    "\n",
    "$$\n",
    "f_t = Sigmoid(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "\n",
    "#### Calculate $o_t$ (output of output gate) : \n",
    "\n",
    "$$\n",
    "o_t = Sigmoid(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "\n",
    "#### Calculate $C_t$ (cell state) : \n",
    "\n",
    "$$\n",
    "\\bar C_t = tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) \\\\\n",
    "C_t = f_t * C_{t-1} + i_t * \\bar C_t\n",
    "$$\n",
    "\n",
    "#### Calculate $h_t$ (hidden state) : \n",
    "\n",
    "$$\n",
    "h_t = o_t * tanh(C_t)\n",
    "$$\n",
    "\n",
    "### 3.2 Model Architecture\n",
    "\n",
    "![](https://i.imgur.com/7TPMPnR.png)\n",
    "\n",
    "\n",
    "Here, we show a simple stacked LSTM model.\n",
    "\n",
    "- $h_{11}, h_{12}, ..., h_{1n}$ , $h_{21}, h_{22}, ..., h_{2n}$, $h_{3n}$ are 1D vectors. They are hidden states of each time stamp.\n",
    "- The first two LSTM layers **return all hidden states**. And the last LSTM layer **only return the final hidden state**.\n",
    "- We will feed the final hidden state $h_{3n}$ to a neural network.\n",
    "\n",
    "We can unroll this architecture as follows : \n",
    "\n",
    "![](https://i.imgur.com/PcIiZEo.png)\n",
    "\n",
    "\n",
    "### 3.3 Bidirectional LSTM\n",
    "\n",
    "A basic problem for all recurrent architectures : \n",
    "- They will **forget** the features input at the beginning.\n",
    "    - This will be a problem if we pad all sequences to the same size.\n",
    "\n",
    "For example, consider raw data as follows : \n",
    "1. I am darby. I am handsome.\n",
    "2. How are you ?\n",
    "3. To be or not to be, this is a question.\n",
    "\n",
    "In practice, we will pad those sequences : \n",
    "1. i am darby i am handsome [pad] [pad] [pad] [pad]\n",
    "2. how are you [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
    "3. to be or not to be this is a question\n",
    "\n",
    "You will find that sample 2 has seven [pad] tokens. This will affect model inference.\n",
    "\n",
    "#### Solution : \n",
    "1. Reverse sequence order\n",
    "2. Bidirectional LSTM\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*6QnPUSv_t9BY9Fv8_aLb-Q.png)\n",
    "\n",
    "## 4. Sequence to Sequence Learning\n",
    "\n",
    "Paper : [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
    "\n",
    "Definition : feed a sequence to model and the model will response with a sequence. For example, chatbot, machine translation.\n",
    "\n",
    "- It is not a simple classification or regression model. Instead, it's a **generative model**.\n",
    "\n",
    "### 4.1 Limitation of generative problem on a RNN model\n",
    "\n",
    "Is it possible that we let each hidden states become an output sequence ?\n",
    "\n",
    "![](https://i.imgur.com/19GryC6.png)\n",
    "\n",
    "- No, because we may consider whole input sequence to generate a response.\n",
    "\n",
    "### 4.2 Encoder-Decoder Architecture\n",
    "\n",
    "Idea : use a RNN model to encode whole input sequence. And decode to a response by another RNN model.\n",
    "- Sequence-to-sequence model (seq2seq)\n",
    "\n",
    "![](https://i.imgur.com/V8rxbS5.png)\n",
    "\n",
    "1. For encoder part, feed whole input sequence to encoder model. And we will get the final cell state and hidden state\n",
    "2. For decoder part, we **initialize** decoder cell state and hidden state by encoder cell state and hidden state.\n",
    "3. Finally, we feed a **start token** to decoder. And we can output a word.\n",
    "    - We feed this word to decoder to get next word. Repeat this process until we get an **end token**.\n",
    "\n",
    "### 4.3 Input and Output Encoding\n",
    "\n",
    "Input : feel free to use any kind of word-level encoding method\n",
    "- One-hot\n",
    "- Word embedding (memory-friendly)\n",
    "\n",
    "Output : only consider **one-hot** encoding.\n",
    "- Seq2seq model will predict the classification of each word.\n",
    "- If we have total vocabulary size 80000, we can expect that the output layer of decoder has 80000 neurons. \n",
    "\n",
    "Think : larger vocabulary size, more memory usage.\n",
    "\n",
    "### 4.4 More Issues\n",
    "\n",
    "- Optimize encoder : reverse sequence order.\n",
    "- Optimize decoding process : beam search algorithm.\n",
    "\n",
    "## 5. Attention Mechanism\n",
    "\n",
    "Let's review seq2seq model : \n",
    "\n",
    "![](https://i.imgur.com/KCmJPN6.png)\n",
    "\n",
    "For the first generated word \"I\", we can know that it is generated by token \"[start]\", hidden state $h_0$, cell state $C_0$.\n",
    "- Also, $h_1, C_1$ will be generated.\n",
    "\n",
    "The second word \"am\" is generated by \"I\", $h_1, C_1$.\n",
    "- $h_2, C_2$ will be generated, too.\n",
    "\n",
    "Summary : $Word_t$ will be generated by $h_{t-1}, C_{t-1}, Word_{t-1}$.\n",
    "- $Word_t$ may depend on state $t-n$ or state $t+n$.\n",
    "- Differnet words has different dependencies.\n",
    "- For example, consider the following machine translation : \n",
    "    - I woke up at 8:00 a.m today. => 我今天早上8點起床\n",
    "    - The order of action and time is reverse. The second Chinese vocaulary \"今天\" depends on the final English word \"today\".\n",
    "\n",
    "### 5.1 Architecture Overview\n",
    "\n",
    "![](https://i.imgur.com/trY1JmW.png)\n",
    "\n",
    "A new conception : **context vector**\n",
    "\n",
    "### 5.2 Context Vector\n",
    "\n",
    "Hidden states of encoder may be important.\n",
    "- Generate a context vector which is a **weighted sum** of all hidden state of encoder.\n",
    "\n",
    "Where are weights of weighted sum from ?\n",
    "- Calculate with **current hidden state of decoder**.\n",
    "\n",
    "Consider all hidden states of encoder $he_1, he_2, ... he_n$ and current hidden state of decoder $hd_t$. We can their weights by the following equation : \n",
    "\n",
    "$$\n",
    "u^t_i = score(he_i, hd_t) \\\\\n",
    "U^t = [u^t_1, u^t_2, u^t_3, ... u^t_n] \\\\\n",
    "a^t = softmax(U^t) \\\\\n",
    "c_t = \\sum a^t_i \\times he_i\n",
    "$$\n",
    "\n",
    "$score(.)$ is a function to calcuate the relation between $he_i$ and $hd_t$. For example : \n",
    "\n",
    "$$\n",
    "score(he_i, hd_t) = v^T tanh(W_e \\cdot he_i + W_d \\cdot hd_t)\n",
    "$$\n",
    "\n",
    "- $v^T$ : a weight matrices\n",
    "- $W_e, W_d$ : trainable variables\n",
    "\n",
    "### 5.3 Prediction\n",
    "\n",
    "Prediction will be calculated by **context vector** and **current hidden state of decoder**.\n",
    "\n",
    "$$\n",
    "\\bar {hd}_t = tanh(W_c \\cdot [c_t; hd_t]) \\\\\n",
    "y_t = softmax(W_s \\cdot \\bar {hd}_t)\n",
    "$$\n",
    "\n",
    "### 5.4 More\n",
    "\n",
    "Two kinds of attention layer : \n",
    "- Global attention\n",
    "- Local attention\n",
    "\n",
    "Attention on CNN model : \n",
    "- Self attention : calculate context vector without hidden states of encoder.\n",
    "\n",
    "Machine Translation with attention mechanism but without CNN and RNN : \n",
    "- Paper : [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [A Gentle Introduction to Backpropagation Through Time](https://machinelearningmastery.com/gentle-introduction-backpropagation-time/)\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Attention Mechanism(Seq2Seq)](https://www.slideshare.net/healess/attention-mechanismseq2seq)\n",
    "- [Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "- [Understanding Bidirectional RNN in PyTorch](https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66)\n",
    "\n",
    "## Appendix\n",
    "\n",
    "Here, we introduce some interesting topics on deep learning.\n",
    "\n",
    "### A. Generative Adversarial Networks (GAN)\n",
    "\n",
    "![](https://image.ibb.co/h5HpoA/gan-reading-list-header-image.jpg)\n",
    "\n",
    "Train two models : generator, discriminator\n",
    "- Generator generates fake images.\n",
    "- Discriminator check if the image is real or not.\n",
    "\n",
    "Goal : Get a generator that will generate fake images.\n",
    "\n",
    "[Meow Generator](https://ajolicoeur.wordpress.com/cats/)\n",
    "\n",
    "![](https://ajolicoeur.files.wordpress.com/2017/07/wgan_1408epoch.png)\n",
    "\n",
    "More : \n",
    "- Conditional GAN\n",
    "- Cycle GAN (Two-domain transfer)\n",
    "- Star GAN (Multi-domain transfer)\n",
    "\n",
    "### B. Deep Reinforcement Learning\n",
    "\n",
    "![](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n",
    "\n",
    "Learn a agent which can act with a specific environment very well.\n",
    "- AlphaGo\n",
    "- Self-driving cars\n",
    "\n",
    "Two strategies to act with environments : \n",
    "- Model-based methods\n",
    "- Model-free methods\n",
    "\n",
    "Three strategies to train agents : \n",
    "- Value-based methods : q-learning\n",
    "- Policy-based methods : policy gradient\n",
    "- Combine value-based and policy-based methods : actor-critic\n",
    "\n",
    "### C. Meta Learning\n",
    "\n",
    "How to learn a learner ?\n",
    "\n",
    "Use reinforcement learning to generate deep learning models : \n",
    "- Paper : [Network Architecture Search with Reinforcement Learning](https://arxiv.org/pdf/1611.01578.pdf)\n",
    "- Paper : [Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/pdf/1802.03268.pdf)\n",
    "\n",
    "### D. Security Issues\n",
    "\n",
    "Adversarial sample : a sample which can make model misclassify that human can recognize it very easily.\n",
    "\n",
    "![](https://ml.berkeley.edu/blog/assets/2017-10-31-adversarial-examples/goodfellow.png)\n",
    "\n",
    "- FSGM Attack\n",
    "- Deepfool\n",
    "- One-pixel Attack\n",
    "- Tool : [foolbox](https://github.com/bethgelab/foolbox)\n",
    "\n",
    "### E. Spiking Neural Networks (keyword only)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
